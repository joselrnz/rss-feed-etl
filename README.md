# RSS Feed ETL + Data Processing System

A comprehensive data pipeline for extracting content from RSS feeds, storing them with SCD1 data management, and applying intelligent filtering and processing.

## ğŸš€ Quick Start

```bash
# Run both regional data pipelines
python3 run_job_pipelines.py both

# Run only Texas region data
python3 run_job_pipelines.py texas

# Run only US-wide data
python3 run_job_pipelines.py us
```

## ğŸ“ Repository Structure

```
rss_feed_etl/
â”œâ”€â”€ ğŸ“„ Main Scripts
â”‚   â”œâ”€â”€ run_job_pipelines.py      # Main script - runs complete data pipelines
â”‚   â”œâ”€â”€ run_etl.py                # RSS feed extraction and loading
â”‚   â”œâ”€â”€ run_job_filter.py         # Data filtering and processing
â”‚   â””â”€â”€ run_ats_enrichment.py     # AI-powered content analysis
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ config.yaml           # Texas region configuration
â”‚   â”‚   â””â”€â”€ config_us.yaml        # US region configuration
â”‚   â””â”€â”€ secrets/
â”‚       â””â”€â”€ service_account.json  # Google Sheets credentials
â”‚
â”œâ”€â”€ ğŸ”§ Source Code
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ etl/                  # ETL processing logic
â”‚       â”œâ”€â”€ rss_feed_etl/         # RSS feed handling
â”‚       â”œâ”€â”€ models/               # Data models
â”‚       â””â”€â”€ utils/                # Utility functions
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ README_ETL_FILTER.md  # Detailed technical guide
â”‚   â”‚   â”œâ”€â”€ QUICK_START.md        # Quick reference guide
â”‚   â”‚   â””â”€â”€ USJobFeedsConfig_Template.md
â”‚   â””â”€â”€ examples/                 # Example configurations
â”‚
â”œâ”€â”€ ğŸ“Š Data & Logs
â”‚   â”œâ”€â”€ logs/                     # Processing logs
â”‚   â”œâ”€â”€ resume.pdf               # Your resume for ATS matching
â”‚   â”œâ”€â”€ skills.json              # Skills database
â”‚   â””â”€â”€ models.json              # AI model configurations
â”‚
â””â”€â”€ ğŸ“‹ Project Files
    â”œâ”€â”€ requirements.txt          # Python dependencies
    â”œâ”€â”€ setup.py                 # Package setup
    â””â”€â”€ tests/                   # Unit tests
```

## ğŸ¯ Core Features

- **ğŸ”„ SCD1 Data Management** - Never lose data, automatic deduplication
- **ğŸŒ Multi-Region Support** - Separate Texas and US data pipelines
- **ğŸ¯ Smart Filtering** - Remove unwanted content, empty descriptions
- **ğŸ¤– AI Content Analysis** - Intelligent content enrichment and scoring
- **ğŸ“Š Google Sheets Integration** - Direct data loading and management
- **â° Timestamp Tracking** - AS_OF_DT for audit trails

## ğŸ“Š Data Flow

### Texas Region Pipeline
```
DataFeedsConfig â†’ ETL â†’ StageData â†’ Filter â†’ TexasData
```

### US Region Pipeline
```
USDataFeedsConfig â†’ ETL â†’ StageData_US â†’ Filter â†’ USData
```

## ğŸ› ï¸ Setup

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Configure Google Sheets:**
   - Place `service_account.json` in `secrets/`
   - Set `GOOGLE_SPREADSHEET_ID` environment variable

3. **Create RSS feed configs:**
   - `DataFeedsConfig` sheet for Texas region data
   - `USDataFeedsConfig` sheet for US region data

4. **Test the system:**
   ```bash
   python3 run_job_pipelines.py both --dry-run
   ```

## ğŸ“š Documentation

- **[Detailed Guide](docs/README_ETL_FILTER.md)** - Complete technical documentation
- **[Quick Start](docs/QUICK_START.md)** - 5-minute setup guide
- **[Security Guide](docs/SECURITY.md)** - ğŸ”’ Protect your confidential information
- **[Examples](examples/)** - Sample configurations

## ğŸ¯ Common Commands

```bash
# Daily data processing routine
python3 run_job_pipelines.py both

# Get last 3 days of data only
python3 run_job_pipelines.py both --days-back 3

# Overwrite existing processed data
python3 run_job_pipelines.py both --loading-mode overwrite

# Run AI content analysis
python3 run_ats_enrichment.py

# Test without changes
python3 run_job_pipelines.py both --dry-run
```

## ğŸ”§ Configuration

Edit `config/config.yaml` and `config/config_us.yaml` to customize:
- RSS feed sources
- Keyword filters
- Date ranges
- Output worksheets

## ğŸ“ˆ Output

The system creates these Google Sheets worksheets:
- **StageData** / **StageData_US** - Raw RSS feed data
- **TexasData** / **USData** - Filtered and processed content
- **Enriched data** - AI-analyzed content with scores

---

**Happy Data Processing!** ğŸš€